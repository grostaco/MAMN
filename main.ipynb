{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFFN(nn.Module):\n",
    "    def __init__(self, d_hidden: int, d_ff: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense1 = nn.Linear(d_hidden, d_ff)\n",
    "        self.dense2 = nn.Linear(d_ff, d_hidden)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.dense1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dense2(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class IntraAttentionStack(nn.Module):\n",
    "    def __init__(self, d_hidden: int, num_heads: int, layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            IntraAttentionBlock(d_hidden, d_hidden, num_heads)\n",
    "            for _ in range(layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, emb: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            emb = layer(emb)\n",
    "\n",
    "        return emb \n",
    "\n",
    "class IntraAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_hidden: int, d_ff: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(d_hidden, num_heads, dropout=.2, batch_first=True)\n",
    "        self.pffn = PointWiseFFN(d_hidden, d_ff) \n",
    "    \n",
    "    def forward(self, emb: torch.Tensor):\n",
    "        attn_output, _ = self.attn(emb, emb, emb, need_weights=False)\n",
    "        h = self.pffn(attn_output)\n",
    "\n",
    "        return h\n",
    "    \n",
    "class GlobalAttention(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features \n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    # TODO: implement biases\n",
    "    def forward(self, h_cw: torch.Tensor, h_ap: torch.Tensor, only_weights = False):\n",
    "        if h_cw.dim() == 3:\n",
    "            logits = F.tanh(h_cw @ self.weights @\n",
    "                            h_ap.swapaxes(1, 2))  # + self.bias\n",
    "        else:\n",
    "            logits = F.tanh(h_cw @ self.weights @ h_ap.swapaxes(1, 2))  # + self.bias\n",
    "\n",
    "        I_attn = F.softmax(logits, \n",
    "                           dim=-1)\n",
    "        \n",
    "        if only_weights:\n",
    "            return I_attn.squeeze(1)\n",
    "        return I_attn @ h_cw\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}'.format(\n",
    "            self.in_features, self.out_features\n",
    "        )\n",
    "\n",
    "\n",
    "def wdmc(h_cp: torch.Tensor, a_range: tuple[tuple[int, int], ...], window_size: int):\n",
    "    tensors = []\n",
    "\n",
    "    for a_s, a_e in a_range:\n",
    "        n = h_cp.size(1)\n",
    "\n",
    "        d = torch.arange(window_size / 2, max(a_s, n - a_e) - 1) + 1\n",
    "\n",
    "        d_weighted = 1 - (d - (window_size/2))/n\n",
    "\n",
    "        r_s = int(a_s - window_size / 2)\n",
    "        r_e = int(n - a_e - window_size / 2 - 1)\n",
    "\n",
    "        tensors.append(torch.cat((d_weighted[:r_s].flip(-1), torch.ones(\n",
    "            window_size + a_e - a_s + 1), d_weighted[-r_e:])).view(-1, 1).repeat(1, 1, h_cp.size(-1)))\n",
    "    return torch.cat(tensors) * h_cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMN(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, d_hidden: int, num_heads: int, layers: int, window_size: int,\n",
    "                 num_labels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings, d_hidden)\n",
    "        self.intra_attn_layers = IntraAttentionStack(d_hidden, num_heads, layers)\n",
    "\n",
    "        self.window_size = window_size \n",
    "        self.global_attn = GlobalAttention(d_hidden, d_hidden)\n",
    "\n",
    "        self.dense = nn.Linear(d_hidden, num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, context: torch.LongTensor, \n",
    "                aspects: torch.LongTensor,\n",
    "                a_ranges: tuple[tuple[int, int], ...]):\n",
    "        context_emb = self.embedding(context)\n",
    "        aspects_emb = self.embedding(aspects)\n",
    "\n",
    "        h_cp = self.intra_attn_layers(context_emb)\n",
    "        h_ap = self.intra_attn_layers(aspects_emb)\n",
    "\n",
    "        h_cw = wdmc(h_cp, a_ranges, self.window_size)\n",
    "        g = self.global_attn(h_cw, h_ap) \n",
    "\n",
    "        h_cw_avg = torch.mean(h_cw, dim=1)\n",
    "\n",
    "        attn_weights = self.global_attn(h_cw_avg, h_ap, only_weights=True)\n",
    "\n",
    "        O = (attn_weights @ g).squeeze(1)\n",
    "        logits = F.tanh(self.dense(O))\n",
    "\n",
    "        return F.softmax(logits, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 768]) torch.Size([768, 768]) torch.Size([1, 80, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3216, 0.3348, 0.3436]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = MAMN(30522, 768, 4, 2, 8, 3)(torch.arange(80).long().unsqueeze(0), torch.arange(80).long().unsqueeze(0), ((20, 20),))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 80])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 80, 768)\n",
    "b = torch.randn(768, 768)\n",
    "c = torch.randn(1, 3, 768)\n",
    "\n",
    "(c @ b @ a.swapaxes(1, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 440M/440M [07:01<00:00, 1.05MB/s] \n",
      "Found cached dataset laptops-trial (C:/Users/User/.cache/huggingface/datasets/grostaco___laptops-trial/default/0.0.0/330ba984e4d7218c66e6e89063270f8c480a86a2c58afa0f854519ce925c5330)\n",
      "100%|██████████| 3/3 [00:00<00:00, 749.30it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('grostaco/laptops-trial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "def tokenize_aspects(aspects):\n",
    "    tokenized = tokenizer(aspects)\n",
    "    \n",
    "    return {f'aspect_{k}': v for k, v in tokenized.items()}\n",
    "\n",
    "dataset = dataset.map(tokenizer, input_columns='content', batched=True)\n",
    "dataset = dataset.map(tokenize_aspects, input_columns='aspect', batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, PretrainedConfig, PreTrainedModel, Trainer, TrainingArguments, DataCollatorWithPadding, default_data_collator, EvalPrediction\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset laptops-trial (C:/Users/User/.cache/huggingface/datasets/grostaco___laptops-trial/default/0.0.0/ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 82.99it/s]\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('grostaco/laptops-trial')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFFN(nn.Module):\n",
    "    def __init__(self, d_hidden: int, d_ff: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense1 = nn.Linear(d_hidden, d_ff)\n",
    "        self.dense2 = nn.Linear(d_ff, d_hidden)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.dense1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dense2(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class IntraAttentionStack(nn.Module):\n",
    "    def __init__(self, d_hidden: int, num_heads: int, layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            IntraAttentionBlock(d_hidden, d_hidden, num_heads)\n",
    "            for _ in range(layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, emb: torch.Tensor, attn_mask = None):\n",
    "        for layer in self.layers:\n",
    "            emb = layer(emb, attn_mask=attn_mask)\n",
    "\n",
    "        return emb \n",
    "\n",
    "class IntraAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_hidden: int, d_ff: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(d_hidden, num_heads, dropout=.2, batch_first=True)\n",
    "        self.pffn = PointWiseFFN(d_hidden, d_ff) \n",
    "    \n",
    "    def forward(self, emb: torch.Tensor, attn_mask = None):\n",
    "        attn_output, _ = self.attn(emb, emb, emb, need_weights=False, key_padding_mask=attn_mask)\n",
    "        h = self.pffn(attn_output)\n",
    "\n",
    "        return h\n",
    "    \n",
    "class GlobalAttention(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features \n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        #self.bias = nn.Parameter(torch.randn(in_features, out_features))\n",
    "    \n",
    "    # TODO: implement biases\n",
    "    def forward(self, h_cw: torch.Tensor, h_ap: torch.Tensor, only_weights = False):\n",
    "\n",
    "        logits = F.tanh(h_ap @ self.weight @\n",
    "                        h_cw.swapaxes(1, 2))  # + self.bias\n",
    "        I_attn = F.softmax(logits, \n",
    "                           dim=-1)\n",
    "        \n",
    "        if only_weights:\n",
    "            return I_attn\n",
    "        return I_attn @ h_cw\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}'.format(\n",
    "            self.in_features, self.out_features\n",
    "        )\n",
    "\n",
    "\n",
    "def wdmc(h_cp: torch.Tensor, a_starts: tuple[int, ...], a_ends: tuple[int, ...], window_size: int):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        h_cp (torch.Tensor): _description_\n",
    "        a_range (tuple[tuple[int, int], ...]): _description_\n",
    "        window_size (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    tensors = []\n",
    "\n",
    "    for a_s, a_e in zip(a_starts, a_ends):\n",
    "        r = []\n",
    "        n = h_cp.size(1)\n",
    "\n",
    "        if a_s - window_size/2 > 0:\n",
    "            d_fwd = torch.arange(a_s - window_size/2, 0, -1)\n",
    "            d_fwd = (1 - d_fwd/n)\n",
    "\n",
    "            r.append(d_fwd)\n",
    "\n",
    "        r.append(torch.ones(min(a_s, window_size//2) +\n",
    "                            min(n - a_e - 1, window_size//2) + a_e - a_s + 1))\n",
    "\n",
    "        if a_e + window_size/2 + 1 < n:\n",
    "            d_bwd = torch.arange(1, n - a_e - window_size/2)\n",
    "            d_bwd = (1 - d_bwd/n)\n",
    "            r.append(d_bwd)\n",
    "\n",
    "        tensors.append(torch.cat(r).view(-1, 1).repeat(1, 1, h_cp.size(-1)))\n",
    "\n",
    "    return torch.cat(tensors).to(h_cp.device) * h_cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMNConfig(PretrainedConfig):\n",
    "    model_type = 'MAMN'\n",
    "\n",
    "    def __init__(self, num_embeddings: int = 30522, d_hidden: int = 768, num_heads: int = 4,\n",
    "                 layers: int = 4, window_size: int = 8, **kwargs):\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.d_hidden = d_hidden \n",
    "        self.num_heads = num_heads \n",
    "        self.layers = layers \n",
    "        self.window_size = window_size\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class MAMN(PreTrainedModel):\n",
    "    config_class = MAMNConfig \n",
    "\n",
    "    def __init__(self, config: MAMNConfig):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.embedding = nn.Embedding(config.num_embeddings, config.d_hidden)\n",
    "        self.intra_attn_layers1 = IntraAttentionStack(\n",
    "            config.d_hidden, config.num_heads, config.layers)\n",
    "        self.intra_attn_layers2 = IntraAttentionStack(\n",
    "            config.d_hidden, config.num_heads, config.layers)\n",
    "\n",
    "        self.window_size = config.window_size\n",
    "        self.global_attn1 = GlobalAttention(config.d_hidden, config.d_hidden)\n",
    "        self.global_attn2 = GlobalAttention(config.d_hidden, config.d_hidden)\n",
    "\n",
    "        self.dense = nn.Linear(config.d_hidden, config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor, \n",
    "                aspects_input_ids: torch.LongTensor,\n",
    "                attention_mask: torch.LongTensor,\n",
    "                aspects_attention_mask: torch.LongTensor,\n",
    "                start: tuple[int, ...], \n",
    "                end: tuple[int, ...],\n",
    "                labels = None,\n",
    "                **kwargs):\n",
    "        context_emb = self.embedding(input_ids)\n",
    "        aspects_emb = self.embedding(aspects_input_ids)\n",
    "\n",
    "        h_cp = self.intra_attn_layers1(context_emb, attn_mask=attention_mask.bool() if attention_mask is not None else None)\n",
    "        h_ap = self.intra_attn_layers2(\n",
    "            aspects_emb, attn_mask=aspects_attention_mask.bool() if attention_mask is not None else None)\n",
    "\n",
    "        #print(f'h_cp: {h_cp.shape} {torch.isnan(h_cp).any()}')\n",
    "        h_cw = wdmc(h_cp, start, end, self.window_size)\n",
    "        \n",
    "        #print(f'h_cw: {h_cw.shape} {torch.isnan(h_cw).any()}')\n",
    "        g = self.global_attn1(h_cw, h_ap) #* aspects_attention_mask\n",
    "\n",
    "        #print(f'g: {g.shape} {torch.isnan(g).any()}')\n",
    "\n",
    "        h_cw_avg = torch.mean(h_cw, dim=1) #* attention_mask\n",
    "        #print(f'h_cw_avg: {h_cw_avg.shape} {torch.isnan(h_cw_avg).any()}')\n",
    "\n",
    "        attn_weights = self.global_attn2(h_ap, h_cw_avg, only_weights=True)\n",
    "        # print(\n",
    "        #     f'attn_weights: {attn_weights.shape} {torch.isnan(attn_weights).any()}')\n",
    "\n",
    "        O = (attn_weights @ g)\n",
    "        logits = F.tanh(self.dense(O))[:, 0]\n",
    "\n",
    "        #print(logits.shape)\n",
    "\n",
    "        loss = None \n",
    "        if labels is not None:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits, \n",
    "            attentions=attn_weights\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        if isinstance(module, nn.Linear | GlobalAttention):\n",
    "            torch.nn.init.uniform(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = dataset['train'].features['labels']._str2int\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "config = MAMNConfig(label2id=label2id, id2label=id2label)\n",
    "model = MAMN(config)\n",
    "model.embedding.weight = bert.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-d8b2b0b0d98ff577.arrow\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-f9857aefe704468a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-309cc52e379f16ca.arrow\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-0a8afeac1ec0ff91.arrow\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-92d4b7e0511eac5e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-3ab66998cea36f52.arrow\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-9754d92f014559db.arrow\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-ebb3ef1ac78fe801.arrow\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\grostaco___laptops-trial\\default\\0.0.0\\ca5733dfc0f9290466b24cc18c4981e2c3f639aa23138bec1753f67f23cb530a\\cache-a500c83624429ed7.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_aspects(aspects):\n",
    "    tokenized = tokenizer(aspects, padding='max_length', truncation=True, max_length=16, return_tensors='pt')\n",
    "    \n",
    "    return {f'aspects_{k}': v for k, v in tokenized.items()}\n",
    "\n",
    "def tokenize(contents):\n",
    "    return tokenizer(contents, padding='max_length', truncation=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize, input_columns='content', batched=True)\n",
    "dataset = dataset.map(tokenize_aspects, input_columns='aspect', batched=True)\n",
    "dataset = dataset.filter(lambda end: end < 80, input_columns='end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0333, -0.0240, -0.0156],\n",
       "        [-0.0331, -0.0241, -0.0156],\n",
       "        [-0.0317, -0.0257, -0.0164]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = 3\n",
    "\n",
    "contexts = torch.tensor(dataset['train']['input_ids'][:samples], dtype=torch.long) \n",
    "aspects = torch.tensor(dataset['train']['aspects_input_ids'][:samples], dtype=torch.long)\n",
    "attention_mask = torch.tensor(dataset['train']['attention_mask'][:samples], dtype=torch.long)\n",
    "aspects_attention_mask = torch.tensor(dataset['train']['aspects_attention_mask'][:samples], dtype=torch.long)\n",
    "starts = dataset['train']['start'][:samples]\n",
    "ends = dataset['train']['end'][:samples]\n",
    "model.to('cpu').forward(contexts, aspects,\n",
    "                        attention_mask, aspects_attention_mask, starts, ends).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.51it/s]c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                             \n",
      "100%|██████████| 8/8 [00:03<00:00,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00        58\n",
      "    negative       0.38      1.00      0.55        48\n",
      "     neutral       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.38       128\n",
      "   macro avg       0.12      0.33      0.18       128\n",
      "weighted avg       0.14      0.38      0.20       128\n",
      "\n",
      "{'eval_loss': 1.0969144105911255, 'eval_accuracy': 0.375, 'eval_precision': 0.140625, 'eval_f1': 0.20454545454545453, 'eval_recall': 0.375, 'eval_runtime': 0.9281, 'eval_samples_per_second': 137.914, 'eval_steps_per_second': 8.62, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:13<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 13.3253, 'train_samples_per_second': 9.606, 'train_steps_per_second': 0.6, 'train_loss': 1.0981285572052002, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=1.0981285572052002, metrics={'train_runtime': 13.3253, 'train_samples_per_second': 9.606, 'train_steps_per_second': 0.6, 'train_loss': 1.0981285572052002, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    y_pred = p.predictions[0].argmax(-1)\n",
    "    print(y_pred)\n",
    "    y_true = p.label_ids\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(\n",
    "        y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=label2id.keys())\n",
    "\n",
    "    print(report)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='mamn',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset['train'].select(range(128)),\n",
    "    eval_dataset=dataset['validation'].select(range(128)),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "100%|██████████| 2/2 [00:00<00:00, 15.45it/s]c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00        10\n",
      "    negative       0.50      1.00      0.67        16\n",
      "     neutral       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.50        32\n",
      "   macro avg       0.17      0.33      0.22        32\n",
      "weighted avg       0.25      0.50      0.33        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0960354804992676,\n",
       " 'eval_accuracy': 0.5,\n",
       " 'eval_precision': 0.25,\n",
       " 'eval_f1': 0.3333333333333333,\n",
       " 'eval_recall': 0.5,\n",
       " 'eval_runtime': 0.4597,\n",
       " 'eval_samples_per_second': 69.608,\n",
       " 'eval_steps_per_second': 4.351,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset['test'].select(range(32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
